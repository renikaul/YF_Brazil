---
title: "Variable Selection and Univariate Analysis"
author: "Michelle Evans"
date: "June 1, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(tidyverse)
library(corrplot)
library(PerformanceAnalytics)
library(Hmisc)
library(ROCR)
```

This document explores collinearity between covariates and then uses a univariate analysis of each to provide support for variable selection.

# Load Covariates

1. load cleaned data that Reni made in giantdfmaker (training only)
2. load extra covariates for temp and rainfall
3. subset to match training data
4. transform as necessary (temp: none, rf: cube root transformed (`TranCube`))

```{r}
#cleaned data
train.data <- readRDS("../data_clean/FinalData_Mosi/Infection/TrainingData.rds")

# extra environmental covariates
rf <- readRDS("../data_clean/environmental/may2018/allRainfall.rds")
temper <- readRDS("../data_clean/environmental/may2018/allTemperature.rds")
```

Function to transform data.

```{r}
TranCube = function(x) {sign(x) * abs(x)^(1/3)}
```


Join to cleaned data

```{r}
all.data <- train.data %>%
  left_join(temper, by = c("muni.no", "month.no")) %>%
  left_join(rf, by = c("muni.no", "month.no")) %>%
  #transform rf
  mutate(rf.min = TranCube(hourlyRainfallMin)) %>%
  mutate(rf.max = TranCube(hourlyRainfallMax)) %>%
  dplyr::select(-numCase, -year.x, -cal.month.x, -muni.name, -year.y, -cal.month.y, 
                -tempMean, -year, -cal.month, -hourlyRainfallMin, -hourlyRainfallMax, -hourlyRainfallMean)
```

# Collinearity


```{r}
env.vars <- dplyr::select(all.data, -muni.no, -month.no, -case)
cor.mat <- cor(env.vars)

cor.obj <- rcorr(as.matrix(env.vars), type = "pearson")
cor.mat <- cor.obj$r
p.mat <- cor.obj$P
```

Plot it (following styling from this [blog](https://rstudio-pubs-static.s3.amazonaws.com/240657_5157ff98e8204c358b2118fa69162e18.html):

```{r}
col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
```


```{r}

tiff(file = "../data_out/MS_results_revisions/Infection/figures/supplement/correlationMatrix.tiff", width = 1500, height = 1500, units = "px", res = 300, family = "sans", compression = "lzw")

corrplot(cor.mat, method = "color", col = col(200),  
         type = "upper", order = "hclust", 
         addCoef.col = "black", # Add coefficient of correlation
         tl.col = "black", tl.srt = 45, #Text label color and rotation
         # hide correlation coefficient on the principal diagonal
         diag = FALSE, tl.cex = 0.5, number.cex = 0.5
         )

dev.off()
```

# Univariate Analyses

There are some variables that have an over 0.9 correlation with each other. We created univariate logistic regressions on the whole dataset to see which are more explanatory. Note our actual model is a bagged logistic regression, so a single logistic regression may have different results

For this, we will use AIC with a \deltaAIC > 10 denoting a difference amongst models.

## Set Up Bagged Model

```{r}
bagging<-function(form.x.y,training,new.data){
  # modified JP's bagging function 12/1/17 RK 
  # form.x.y the formula for model to use
  # training dataframe containing training data (presence and abs)
  # new.data new data for logreg model to predict onto
  
  # returns predictions based on logreg model for new data and coefficients of model
  #0. load packages
  require(dplyr)
  #1. Create subset of data with fixed number of pres and abs
  training.pres <- dplyr::filter(training, case==1) #pull out just present points
  training.abs <- dplyr::filter(training, case==0)  #pull out just absence points
  training_positions.p <- sample(nrow(training.pres),size=10) #randomly choose 10 present point rows
  training_positions.b <- sample(nrow(training.abs),size=100) #randomly choose 100 absence point rows  
  train_pos.p<-1:nrow(training.pres) %in% training_positions.p #presence 
  train_pos.b<-1:nrow(training.abs) %in% training_positions.b #background
  #2. Build logreg model with subset of data    
  glm_fit<-glm(form.x.y,data=rbind(training.pres[train_pos.p,],training.abs[train_pos.b,]),family=binomial(logit))
  #3. Pull out model coefs  
  #glm.coef <- coef(glm_fit)
  #4. Use model to predict (0,1) on whole training data   
  predictions <- predict(glm_fit,newdata=new.data,type="response")
  return(predictions)
}
```

```{r}
univariateBag <- function(IV, iters = 25, data.set = all.data){
  #' Runs a univariate bagged model 
  
  #' @param IV independent variable to use in univariate analysis
  #' @param iters number of bags to use (default = 25)
  #' @param data.set dataset to train and predict over. This does not use a holdout dataset as it is really just to show that univariate analyses of correlated variables are all the same
  
  #' @results mean and sd of AUC over dataset provided
  
  univar.form <- as.formula(paste0("case ~ ", IV))
  outputs <- matrix(nrow = nrow(data.set), ncol = iters)
  for (i in 1:ncol(outputs)){
    outputs[,i] <- bagging(univar.form, data.set, data.set)
  }
  
  #calculate AUC for each bag
  AUC.df <- numeric(ncol(outputs))
  truth <- data.set$case
  for (i in 1:ncol(outputs)){
    preds <- outputs[,i]
    AUC.obj <- ROCR::prediction(preds, truth)
    AUC.df[i] <- unlist(ROCR::performance(AUC.obj, "auc")@y.values)
  }
  return(list(AUC.mean = mean(AUC.df), AUC.sd = sd(AUC.df)))
}

```
 
AUC function
 
```{r}
# AUC function
calcAUC <- function(model){
  truth <- model$data$case
  preds <- model$fitted.values
  AUC.obj <- ROCR::prediction(preds, truth)
  return(unlist(ROCR::performance(AUC.obj, "auc")@y.values))
}
```

## Temperature

min, mean, and max temp were highly correlated with pearson's \rho values between 0.89 and 0.97.

```{r}
#create models
tmean.mod <- glm(case ~ temp, data = all.data, family = binomial (link = "logit"))
tmin.mod <- glm(case ~ tempMin, data = all.data, family = binomial (link = "logit"))
tmax.mod <- glm(case ~ tempMax, data = all.data, family = binomial (link = "logit"))
```

```{r}
#compare AIC
AIC(tmean.mod, tmin.mod, tmax.mod)
bbmle::AICtab(tmean.mod, tmin.mod, tmax.mod, weights = T)

#compare AUC
calcAUC(tmean.mod)
calcAUC(tmin.mod)
calcAUC(tmax.mod)
```

Or use a bagged approach that matches our own:

```{r}
t.mean.uni.bag <- univariateBag("temp")
t.min.uni.bag <- univariateBag("tempMin")
t.max.uni.bag <- univariateBag("tempMax")
```

## Rainfall

Min, mean, max rf were all highly correlated with pearson's coefficients between 0.97 and 0.99.

```{r}
#create models
rf.mean.mod <- glm(case ~ rf, data = all.data, family = binomial (link = "logit"))
rf.min.mod <- glm(case ~ rf.min, data = all.data, family = binomial (link = "logit"))
rf.max.mod <- glm(case ~ rf.max, data = all.data, family = binomial (link = "logit"))
```

```{r}
#compare AIC
AIC(rf.mean.mod, rf.min.mod, rf.max.mod)
bbmle::AICtab(rf.mean.mod, rf.min.mod, rf.max.mod, weights = T)

#compare AUC
calcAUC(rf.mean.mod)
calcAUC(rf.min.mod)
calcAUC(rf.max.mod)
```

Using a bagged approach:

```{r}
rf.mean.uni.bag <- univariateBag("rf")
rf.min.uni.bag <- univariateBag("rf.min")
rf.max.uni.bag <- univariateBag("rf.max")
```

## Print univariate AUC table

```{r}
AUCresults <- data.frame(cbind(c("variable.type", "Minimum", "Mean", "Maximum"),
                         c("Rainfall", 
                           paste0(round(rf.min.uni.bag$AUC.mean, 3), " (", round(rf.min.uni.bag$AUC.sd,3), ")"), 
                           paste0(round(rf.mean.uni.bag$AUC.mean,3), " (", round(rf.mean.uni.bag$AUC.sd, 3), ")"),
                           paste0(round(rf.max.uni.bag$AUC.mean, 3), " (", round(rf.max.uni.bag$AUC.sd,3), ")")),
                         c("Temperature", 
                           paste0(round(t.min.uni.bag$AUC.mean, 3), " (", round(t.min.uni.bag$AUC.sd, 3), ")"), 
                           paste0(round(t.mean.uni.bag$AUC.mean, 3), " (", round(t.mean.uni.bag$AUC.sd, 3), ")"),
                           paste0(round(t.max.uni.bag$AUC.mean, 3), " (", round(t.max.uni.bag$AUC.sd, 3), ")"))))
```

Save results

```{r}
write.csv(AUCresults, "../data_out/MS_results_revisions/Infection/univariateAUC.csv", row.names = F)
```

