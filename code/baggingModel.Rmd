---
title: "YF Bagged LogReg Model"
author: "Michelle Evans, Spencer Hall, Reni Kaul, Anna Kate Schatz"
date: "July 12, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load packages}

```

# General Outline of Model

## Load, Combine, and Clean/Format Data

Make sure that there is a row for every muni x year x month, not *only* presences.

Population data is log 10 transformed in Ebola work, after correcting 0 to 1 to avoid NAs with log transformation. Between five year increments were interpolated. Population was then binned.

Original ebola paper used a scaled rainfall value (scaled to maximum of muni, in our case). Also used one that was logged transformed. Looks like they used both at the same time?

EVI and PET were rescaled by subtracting the mean and dividing by sd. In the Ebola paper, these were just averages from 2000 - 2014, so there is only one layer/ So this is the mean of the whole layer (in our case all munis at a year x month).

Data was sorted/stratified by rainfall so that the training/test split was not biased by rainfall levels.

2/3 split for training/testing.

### Scaling of Environmental Covariates

Rainfall is of a similar range (0 -1.5), with an exponential distribution. We normalized in two ways. First we log transformed it. Second, we rescaled to the maximum value of that municipality. This resulted in two values for each type of rainfall, for a total of six.  

```{r}
rf <- readRDS("../data_clean/environmental/allRainfall.rds")
hist(rf[,5]) #min rainfall exponential with long tail
range(rf[,5])
hist(rf[,6]) #max rainfall, xponential
range(rf[,6])
hist(rf[,7]) #mean rainfall, exponential
range(rf[,7])

#transformed variables

#log transformed
nonzero<-  min(rf[which(rf[,5]>0),5])#what is smallest non-zero value
hist(log((rf[,5])+nonzero/10)) #check that handling zeros is ok

rf$log.min.rf  <- log((rf[,5])+nonzero/10)
rf$log.max.rf  <- log((rf[,6])+nonzero/10)
rf$log.mean.rf  <- log((rf[,7])+nonzero/10)

#scale to maximum value of muni
library(dplyr)

rf <- rf %>%
  group_by(muni.no) %>%
  mutate(max.max.rf = max(hourlyRainfallMax)) %>%
  mutate(scale.max.rf= hourlyRainfallMax/max.max.rf) %>%
  mutate(max.min.rf = max(hourlyRainfallMin)) %>%
  mutate(scale.min.rf = hourlyRainfallMin/max.min.rf) %>%
  mutate(max.mean.rf = max(hourlyRainfallMean)) %>%
  mutate(scale.mean.rf = hourlyRainfallMean/max.mean.rf) %>%
  select(-max.max.rf, -max.min.rf, -max.mean.rf) %>%
  ungroup()

```

NDVI data is ranged 0 - 1 and is normally distributed, probably doesn't need rescaling. This doesn't need to be trasformed because it is between 0 - 1 and normally distributed.
```{r}
ndvi <- readRDS("../data_clean/environmental/allNDVI.rds")
hist(ndvi[,5]) #normal

ndvi.subset <- ndvi %>%
  group_by(muni.no) %>%
  summarise_each(funs(mean,sd)) 

ndvi.subset <- ndvi.subset[sample(0:5564,1000),]
library(gplots)
plotCI(x=1:999, y=ndvi.subset$NDVI_mean, uiw=ndvi.subset$NDVI_sd, gap=0)

```

Temperature is normally distributed but has a wide range. Could be rescaled by subtracting the mean and dividing by sd per month layer?
```{r}
temp <- readRDS("../data_clean/environmental/allTemperature.rds")
hist(temp$tempMax)
hist(temp$tempMean)
hist(temp$tempMin)

#rescale each month by subtracting mean and dividing by sd (ie standardizing)
stdz <- function(x){
  return((x-mean(x))/sd(x))
}

temp.scale <- temp %>%
  group_by(year, month) %>%
  mutate(tempMin.scale=stdz(tempMin))

```


Population Data

This is where you add in shit. we want to call in population data and log10 transform. and bin!
created 3 population bins according to x<10^2, 10^2<x<10^3, and x>10^3

## Bagging

Function from Ebola:

```{r}
bagging<-function(form.x.y,training.pres,training.abs,new.data,iterations=176){
  #' JP's bagging function
  #' 
  #' @param form.x.y the formula for model to use
  #' @param training.pres dataframe containing training presence data
  #' @param training.abs dataframe containing training background data
  #' @param new.data new data for each logreg model to predict onto
  #' @param iterations number of cores to use. also maybe has something to do with model iterations???
  #'
  #' @returns predictions on new data as a combination (sum??) of predictions from each logreg model
  predictions<-foreach(m=1:9,.combine='+') %dopar% {
    training_positions.p <- sample(nrow(training.pres),size=10)
    training_positions.b <- sample(nrow(training.abs),size=100)
    train_pos.p<-1:nrow(training.pres) %in% training_positions.p #presence
    train_pos.b<-1:nrow(training.abs) %in% training_positions.b #background
    glm_fit<-glm(form.x.y,data=rbind(training.pres[train_pos.p,],training.abs[train_pos.b,]),family=binomial(logit))
    predict(glm_fit,newdata=new.data,type="response")
  }
  return(predictions)
}
```

Example of using above function:

```{r}
#write glm formula
glm.formula<-as.formula("pres~rf.ym+evi+pet+rf.scaled+as.factor(pd.yr.binned)")

#predict on training set
training<-rbind(train.pres,train.bg)
output1<-bagging(form.x.y=glm.formula,training.pres=train.pres,training.abs=train.bg,new.data=training,iterations=176)

#predict on testing set
testing<-rbind(test.pres,test.bg)
output2<-bagging(form.x.y=glm.formula,training.pres=train.pres,training.abs=train.bg,new.data=testing,iterations=cores.to.use)
```

Giving it a test with fake data:

```{r}
trainTest <- cbind(c(rep(1, 30), rep(0,400)), c(rnorm(30,5,2), rnorm(400,10,4)), c(rnorm(30,100,20), rnorm(400,40,8)), c(rnorm(30,0.5, 0.1), rnorm(400,0.1,0.1)))
trainTest[trainTest<0] <- 0
colnames(trainTest) <- c("pres", "v1", "v2", "v3")
trainTest <- as.data.frame(trainTest)

#split into pres and abs
train.pres <- trainTest[trainTest$pres==1,]
train.abs <- trainTest[trainTest$pres==0,]

glm.formula <- as.formula("pres~v1+v2+v3")

cores.to.use <- 1 #bc my macbook sucks
cl <- makeCluster(cores.to.use)
registerDoParallel(cl)

output1 <- bagging(form.x.y=glm.formula,training.pres=train.pres,training.abs=train.abs,new.data=trainTest,iterations=1)

stopCluster(cl) #this worked perfectly (all the positives got 9, and 0s got 0)

```

The bagging model sums up the predictions (in this case 9 of them), which then should be averaged.

After this, they then predicted over all of sub-Saharan Africa:

1. Combined all presence into one df, and all bg into one df

2. did bagged predictions using all p and all bg

3. each iteration was split by year, then run over all the cores, six times each

```{r}
# for (i in 1:12){
#   rf.scaled<-fread("rf.scaled.all.csv",select=((i*32)-31):(i*32)) 
#   rf.ym<-fread("rf.ym.df.csv",select=((i*32)-31):(i*32)) 
#   for (j in 1:32){
#     rf.scaled.step<-rf.scaled.all[,j] #Single year of data
#     rf.ym.step<-rf.ym.all[,j] #Single year of data
#     all.data.step<-as.data.frame(cbind(evi.pet.pd.binned,rf.scaled.step,log10(rf.ym.step+1)))
#     colnames(all.data.step)<-c("evi","pet","pd.yr.binned","rf.scaled","rf.ym")
#     sum<-NULL
#     chunks<-6
#     for (k in 1:chunks){
#       output<-bagging(form.x.y=glm.formula,train.pres=final.p,training.bg=final.b,new.data=all.data.step,iterations=cores.to.use) #what does k do here?? just runs 6 times
#       sum<-sum+output
#     }
#     mean.predict<-sum/(cores.to.use*chunks)
#     writeRDS(mean.predict,file=paste("Bags/predictionsSet",i*j))
#     print(i*j)
#   }
# }
```

## Testing Bagging Performance

Created ROC curves for sens-spec when using all presences, prediction on training data, and prediction on testing data.


### Questions (as of 2017-07-12)

1. Where does the randomness come in? Is each iteration meant to be a model? Or a group of nine models?

2. Are the predictions summed? And then divided by the total number of logreg models to give the mean?

3. What makes each individual model weak? The use of only 110 points (10p + 100 bg)?

4. Seed is never explicitly set in the iterations, but I get the sense that it is different and this what subsets different groups of training and testing data for each glm
