---
title: "YF Bagged LogReg Model"
author: "Michelle Evans, Spencer Hall, Reni Kaul, Anna Kate Schatz"
date: 
header-includes:
    - \usepackage{gensymb}
output: 
  html_document:
      toc: true
      toc_depth: 3
      toc_float: true
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, cache = TRUE, eval = FALSE)
```

```{r load packages}
library(dplyr)
library(gplots)
library(parallel)
library(doParallel)
library(ROCR)
library(pROC)
```


# General Outline of Model

Goal: Create a map 

## Data Summary

Monthly confirmed cases of yellow fever for each Brazilian Munic\'{i}pio (sub-state administrative units) from 2001 to 2014 were downloaded from the Brazilian government's portal da sa\'{u}de website,\href{http://tabnet.datasus.gov.br}{tabnet} on 05 June 2017. Confirmed cases were reported by the Ministry of Health Notification of Injury Information System (Sianan Net) after positive XYZ test. 

The annual population for each Munic\'{i}pio from 2001 to 2014 was also downloaded from the Brazilian government's portal da sa\'{u}de website,\href{http://tabnet.datasus.gov.br}{tabnet} on 05 June 2017. The estimated population was calculated by the Instituto Brasileiro de Geografia e Estat\'{i}stica (Brazialian Institure of Geography and Statistics)   The annual populations from 2001 to 2010 were intercensorial estimates. Post 2010 used slightly different methods ( methods are in Portuguese and pdf form; working on translation).

Monthly land surface temperature and normalized difference vegetation index (NDVI) data from 2001 through 2014 were downloaded from the NASA Land Processes Distributed Active Archive Center (LPDAAC. The MODIS MOD11C3 product contains monthly temperature data at a 0.05$^{\circ}$ resolution. THE MODIS MOD13A3 product contains monthly NDVI data at a 1 km resolution. Both gridded data products were then aggregated to the municipality level to obtain a monthly spatially averaged temperature and NDVI value for each municipality. Rainfall data was obtained from the NASA GESDISC data archive in the form of data from the Tropical Rainfall Monitoring Mission from 2001 through 2014. The 3B43 product contains an average rainfall rate for each month at a 0.25$^{\circ}$ resolution. We aggregated the gridded data to the municipality level by taking the spatial mean.

Both rainfall and NDVI data were further transformed to create fine-scale environmental variables to better characterize environmental anomalies. Rainfall data were scaled to the maximum value of the municipality for the whole period of 2001 to 2014 and NDVI data were scaled to the maximum value for that specific calendar month for each municipality (i.e. June was scaled with all June values from 2001 - 2014).  

##Covariates and rationale (what they should be)

1. Rainfall (ave rate of rainfall per day)
  -sqrt transformed
2. log population density updated annually
3. Monthly NDVI
4. Monthly NDVI scaled to max value at that location and month
5. Monthly mean temperature 
  

# case :reporting of any YF cases (0,1)
# NumCase :number of reported YF cases 
# NDVI : NDVI for that month
# NDVIScale : NDVI rescaled to max value for that muni and calendar month
# muniArea : km2
# popLog10 : population density 
# RF : mean hourly rainfall
# RFsqrt : mean hourly rainfall sqrt transformed
# RFScale : mean hourly rainfall rescaled to max value for that muni and calendar month
# tempMean : mean monthly air temperature 
# tempScale : mean monthly air temperature rescaled to max value for that muni and calendar month
# fireNum : number of fires oberserved in month
# fireDen : number of fires oberserved in month divided by muniArea
# fireDenSqrt : number of fires oberserved in month divided by muniArea sqrt transformed
# firesDenScale : number of fires oberserved in month divided by muniArea rescaled to max value for that muni and calendar month. NA values converted to zero. 
# spRich : number of non-human primates by species with ranges based on IUCN {0-22}
# primProp : sum of each municipalities relative area that is both agricultural and falls within a primate genus range. {0,9} Missing for 2014
# muni.no : unique number to identify municipality
# month.no : numbered month of observation {1,168}
# muni.name : character string name
# month : abbrevated calendar month name
# cal.month : calendar month {1,12}
# year : year {2001,2014}

## Processing Environmental Covariates

   All of the processesing is now done in `GiantDataFrameMaker.R`. 
Infected municipalities were 70/30 split into training and testing data, stratified by year. Background data was also split 70/30.  

```{r}
training.data <- readRDS("../data_clean/TrainingData.rds")

```   
   
### Rainfall


Rainfall ranges from (`r signif(range(rf$hourlyRainfallMean), digits=2)`), and is exponentially distribution. The mean rainfall was ln transformed. A second covariate, scaled mean rainfall, was created by scalling rainfall to the maximum value of that municipality. We will only use anomalous mean rainfall (scale.mean.rf) and mean rainfall (log.mean.rf) for each municipality. 

### NDVI


NDVI data ranges from 0 - 1 and is normally distributed. This covariate will be used "as-is", and re-scaled to the maximium observed value for that municipality and month. Given NDVI should change with season, we scaled NDVI for values within a given month (14 months). The overall range in NDVI for a municipality and calendar month is right skewed with a mean of `r signif(mean(ndvi$range.ndvi, na.rm = TRUE), digits=2)`.  There is currently an issue with Madre de Deus (291992) municipality returning NA values. This municipality was never infected, and has been dropped for now.  

### Temperature
Temperature is normally distributed, and was used "as-is".

#### Population Data

Because the range is huge, we log10 transformed (population +1). We also need to adjust the population data for the muncipalities that recently became emancipated in 2013 (based on `muniCorrection.Rmd`).



### Case Data
  There are a total of 117 *infected* municipalities in the entire dataset. See `data_clean/case_sparsity.html` for more details. 

## Model

```{r bagging functions}
# Bagging ----
bagging<-function(form.x.y,training,new.data){
  # modified JP's bagging function 12/1/17 RK 
  # form.x.y the formula for model to use
  # training dataframe containing training data (presence and abs)
  # new.data new data for logreg model to predict onto
  
  # returns predictions based on logreg model for new data and coefficients of model
  #0. load packages
  library(dplyr)
  #1. Create subset of data with fixed number of pres and abs
  training.pres <- dplyr::filter(training, case==1) #pull out just present points
  training.abs <- dplyr::filter(training, case==0)  #pull out just absence points
  training_positions.p <- sample(nrow(training.pres),size=10) #randomly choose 10 present point rows
  training_positions.b <- sample(nrow(training.abs),size=100) #randomly choose 100 absence point rows  
  train_pos.p<-1:nrow(training.pres) %in% training_positions.p #presence 
  train_pos.b<-1:nrow(training.abs) %in% training_positions.b #background
  #2. Build logreg model with subset of data    
  glm_fit<-glm(form.x.y,data=rbind(training.pres[train_pos.p,],training.abs[train_pos.b,]),family=binomial(logit))
  #3. Pull out model coefs  
  glm.coef <- coef(glm_fit)
  #4. Use model to predict (0,1) on whole training data   
  predictions <- predict(glm_fit,newdata=new.data,type="response")
  return(list(predictions, glm.coef))
}

#Bagged model in parallel ----
# Permute One Variable at a time ----
baggedlogistic=function(formula = glm.formula, bag.fnc=bagging,traindata = training,  cores=2, no.iterations= 100){
  # glm.formula:
  # training : training data with pres and abs
  # cores : number of cores to use for parallel; default to 2
  # no.iterations : number of low bias models to make; default to 100
  library(dplyr)
  library(doParallel)
  
    #Set up parallel cores to return 2 lists: 1) predictions, 2) coef of model ----
      cores.to.use <- cores #change dep on computer
      iterations <- no.iterations #number of low bias models to make
      
      #create class to combine multiple results
      multiResultClass <- function(predictions = NULL,coefs = NULL)
      {
        me <- list(
          predictions = predictions,
          coefs = coefs
        )
        
        ## Set the name for the class
        class(me) <- append(class(me),"multiResultClass")
        return(me)
      }
      
      
      cl <- makeCluster(cores.to.use)
      registerDoParallel(cl)
      
      trainModel <- foreach(i=1:iterations) %dopar% {
        result <- multiResultClass()
        output1 <- bag.fnc(form.x.y=formula,training= traindata ,new.data=traindata)
        result$predictions <-output1[[1]]
        result$coefs <- output1[[2]]
        return(result)
      }
      
      stopCluster(cl)
      #aggregate data from clusters ----
      #pull out data in usable fashion
      trainingPreds <- do.call(cbind,(lapply(trainModel, '[[', 1)))
      trainingCoefs <- do.call(cbind,(lapply(trainModel, '[[', 2)))
      
    #Calculate AUC of bagged model ----  
      output.preds<- apply(trainingPreds, 1, mean)
      preds <- prediction(output.preds, traindata$case) #other projects have used dismo::evaluate instead. Not sure if is makes a difference. 
      #AUC to return
      train.auc <- unlist(performance(preds, "auc")@y.values)
      
    #Build data frame for plotting ----
      plotPreds <- cbind(muni.no=traindata$muni.no,month.no=traindata$month.no, case=traindata$case, prediction=output.preds)
  
  #return AUC, mean predictions to plot, all predictions, all training coefs
  return(list(train.auc, plotPreds , trainingPreds, trainingCoefs))
}


# Permute Variable based on loop iteration of PermOneVar ----
permutedata=function(formula = glm.formula,trainingdata, i){
  # glm.formula:
  # training : training data with pres and abs
  # cores : number of cores to use for parallel; default to 2
  # no.iterations : number of low bias models to make; default to 100
  
  #parse out variables from formula object 
  variables <- trimws(unlist(strsplit(as.character(formula)[3], "+", fixed = T)), which = "both")
  variablesName <- c("full model", variables, "all permutated")
  
  #if statments to permute data as needed ----
    if(i==1){
      #run full model
      permuted.data <- trainingdata
    }else if(i==length(variablesName)){
      #permute all variables; using loop so can I can use same sampling method (apply statement coherced data into weird format)
      # temp.data <- dplyr::select(traindata, variables) %>%
      #   dplyr::sample_frac() 
      # permuted.data <- cbind(case=traindata$case, tmp.data)
      
      #bug: treating colnames as colnumber in fun but not when ran in console. :(   
      permuted.data <- trainingdata
      for( j in 1:length(variables)){
        vari <- variables[j]
        permuted.data[,vari] <- sample(permuted.data[,vari],dim(permuted.data)[1],FALSE) #permute the col named included in vari (ie. variable.names)
      }   
    } else {
      #permute single variable
      permuted.data <- trainingdata
      permuted.data[,variablesName[i]] <- sample(permuted.data[,variablesName[i]],dim(permuted.data)[1],FALSE) #permute the col named included in vari (ie. variable.names)
    } 
    
    return(permuted.data)
}
  
# Permute One Variable at a time ----
permOneVar=function(formula = glm.formula, bag.fnc=bagging,permute.fnc=permutedata, traindata = training,  cores=2, no.iterations= 100, perm=10){
  # glm.formula:
  # training : training data with pres and abs
  # cores : number of cores to use for parallel; default to 2
  # no.iterations : number of low bias models to make; default to 100
  library(dplyr)
  library(doParallel)
  
  #parse out variables from formula object 
  variables <- trimws(unlist(strsplit(as.character(formula)[3], "+", fixed = T)), which = "both")
  variablesName <- c("full model", variables, "all permutated")
  
  #make objects for outputs to be saved in
  perm.auc <- matrix(NA, nrow=perm, ncol=length(variablesName)) #place to save AUC of models based on different permuation
  
  for (j in 1:length(variablesName)){
    print(c(j,variablesName[j])) #let us know where the simulation is at. 
    VarToPerm <- j
    for (k in 1:perm){
    #if statments to permute data as needed, replaced with permuteddata fnc ----
        permuted.data <- permute.fnc(formula=formula, trainingdata= traindata, i=VarToPerm)  #permute variable of interest
        print(c(j,k))
      
    #Set up parallel cores to return 2 lists: 1) predictions, 2) coef of model ----
      cores.to.use <- cores #change dep on computer
      iterations <- no.iterations #number of low bias models to make
      
      #create class to combine multiple results
      multiResultClass <- function(predictions = NULL,coefs = NULL)
      {
        me <- list(
          predictions = predictions,
          coefs = coefs
        )
        
        ## Set the name for the class
        class(me) <- append(class(me),"multiResultClass")
        return(me)
      }
      
      
      cl <- makeCluster(cores.to.use)
      registerDoParallel(cl)
      
      trainModel <- foreach(i=1:iterations) %dopar% {
        result <- multiResultClass()
        output1 <- bag.fnc(form.x.y=formula,training= permuted.data ,new.data=traindata)
        result$predictions <-output1[[1]]
        #result$coefs <- output1[[2]]
        return(result)
      }
      
      stopCluster(cl)
      #aggregate data from clusters ----
      #pull out data in usable fashion
      trainingPreds <- do.call(cbind,(lapply(trainModel, '[[', 1)))
      #trainingCoefs <- do.call(cbind,(lapply(trainModel, '[[', 2)))
      
      output.preds<- apply(trainingPreds, 1, mean)
      preds <- prediction(output.preds, traindata$case) #other projects have used dismo::evaluate instead. Not sure if is makes a difference. 
      
      #matrix of AUC to return
      perm.auc[k,j] <- unlist(performance(preds, "auc")@y.values)
      
    }
  }
  #calculate relative importance
  perm.auc.mean <- apply(perm.auc,2,mean)
  perm.auc.sd <- apply(perm.auc, 2, sd)
  delta.auc <- perm.auc.mean[1] - perm.auc.mean[-c(1, length(perm.auc.mean))] #change in AUC from base model only for single variable permutation
  rel.import <- delta.auc/max(delta.auc) # normalized relative change in AUC from base model only for single variable permutation
  
  #Output for relative importance
  relative.import <- as.data.frame(cbind(Permutated=variables,RelImport=rel.import))
    #plot it for fun
  barplot(rel.import, names.arg = variables)
  #Output for mean and sd of permutations for all permutations (non, single var, and all var)
  mean.auc <- as.data.frame(cbind(Model=variablesName,meanAUC=perm.auc.mean, sdAUC=perm.auc.sd))
  
  #Output of AUC for each permutation
  colnames(perm.auc) <- variablesName
  
  #return training coefs and AUC for each iteration
  #return(list(train.auc, Coefs))
  return(list(rel.import, mean.auc,perm.auc))
}


```
  
### Testing & Training Split

```{r train model}
glm.formula <- as.formula("case~  NDVI+NDVIScale+ 
                          popLog10+
                          RF+RFsqrt+RFScale+
                          tempMean+tempScale+
                          fireNum+fireDen+fireDenSqrt+fireDenScale+
                          spRich+primProp") 
```

The model is ran for 100 iterations with a subset of the cases. The subset consists of 10 presences (of the 39 training) and 100 background (of the ~300,000) observations. Each iteration of the model is used to predict on the whole training dataset. This is used to calculate AUC. 


### Permutating variables for importance   

  New permutation of data for each of the 100 bagged models. 
Variable importance will be based on change in AUC when variable is permutated. Starting with the all the variables, we will permute one and calculate the AUC on the training data. The baseline AUC was calculated by permuting all the variables. The population density ($Log_{10}$ transformed) seems to be the most important predictor of any YF cases in a municipality. This is expected and doesn't add any intuition to the system. 
    *permutation results variable, why? 
        Expect this when model perf poor, but our AUC is relatively high meaning that perm should always have neg impact
        Does this mean model is overfit; how to tell? (k-folds CV with dif train/test)
        Why the 1:10 proportion. What is risk if we increase, how to know if we have too many pres in balanced data
        
        #rescale AUC for importan  
```{r kitchen sink variable importance}
#drop one covar importance
glm.formula <- as.formula("case~  NDVI+NDVIScale+ 
                          popLog10+
                          RFsqrt+RFScale+
                          tempMean+tempScale+
                          fireDenSqrt+fireDenScale+
                          spRich+primProp") 


PermFullModel <- permOneVar(formula = glm.formula,traindata = training.data, cores=20, no.iterations = 100, perm = 10 )

PermFullAUC <- PermFullModel[[2]]
PermFullAUC[,2] <- round(as.numeric(as.character(PermFullAUC[,2])),digits=3)
PermFullAUC[,3] <- round(as.numeric(as.character(PermFullAUC[,3])),digits=3)

PermFullAUC[order(PermFullAUC$meanAUC),]


```

```{r drop Pop}
#drop one covar importance
glm.formula.popless <- as.formula("case~  NDVI+NDVIScale+ 
                          RFsqrt+RFScale+
                          tempMean+tempScale+
                          fireDenSqrt+fireDenScale+
                          spRich+primProp") 

PoplessVariables <- trimws(unlist(strsplit(as.character(glm.formula.popless)[3], "+", fixed = T)), which = "both")

Sys.time()
PermPoplessModel <- permOneVar(formula = glm.formula.popless,traindata = training.data, cores=20, no.iterations = 1000, perm = 10 ) #increased number of bagged models to 1000 from 100
Sys.time()

PermPoplessAUC <- PermPoplessModel[[2]]
PermPoplessAUC[,2] <- round(as.numeric(as.character(PermPoplessAUC[,2])),digits=3)
PermPoplessAUC[,3] <- round(as.numeric(as.character(PermPoplessAUC[,3])),digits=3)

PermPoplessAUC[order(PermPoplessAUC$meanAUC),]

PermPoplessRelImp <- PermPoplessModel[[1]]
barplot(PermPoplessRelImp, names.arg = PoplessVariables, cex.axis = .75)


```

```{r single flavor}
#permute one covar importance, single flavor (SF) of each covariate
glm.formula.SF <- as.formula("case~  NDVIScale+ 
                          RFsqrt+
                          tempMean+
                          spRich") 

SingleVariables <- trimws(unlist(strsplit(as.character(glm.formula.SF)[3], "+", fixed = T)), which = "both")

Sys.time()
PermSFModel <- permOneVar(formula = glm.formula.SF,traindata = training.data, cores=20, no.iterations = 1000, perm = 10 ) #increased number of bagged models to 1000 from 100
Sys.time()

PermSFAUC <- PermSFModel[[2]]
PermSFAUC[,2] <- round(as.numeric(as.character(PermSFAUC[,2])),digits=3)
PermSFAUC[,3] <- round(as.numeric(as.character(PermSFAUC[,3])),digits=3)

PermSFAUC[order(PermSFAUC$meanAUC),]

PermSFRelImp <- PermSFModel[[1]]
barplot(PermSFRelImp, names.arg = SingleVariables, cex.axis = .75)

saveRDS(PermSFModel, file="../data_out/PermSFModel.rds")
```

```{r single flavor plus pop}
#permute one covar importance, single flavor (SF) of each covariate
glm.formula.SFP <- as.formula("case~  popLog10+ 
                          NDVIScale+ 
                          RFsqrt+
                          tempMean+
                          spRich") 

SFPVariables <- trimws(unlist(strsplit(as.character(glm.formula.SFP)[3], "+", fixed = T)), which = "both")

Sys.time()
PermSFPModel <- permOneVar(formula = glm.formula.SFP,traindata = training.data, cores=20, no.iterations = 1000, perm = 100 ) #increased number of bagged models to 1000 from 100
Sys.time()

PermSFPAUC <- PermSFPModel[[2]]
PermSFPAUC[,2] <- round(as.numeric(as.character(PermSFPAUC[,2])),digits=3)
PermSFPAUC[,3] <- round(as.numeric(as.character(PermSFPAUC[,3])),digits=3)

PermSFPAUC[order(PermSFPAUC$meanAUC),]

PermSFPRelImp <- PermSFPModel[[1]]
barplot(PermSFPRelImp, names.arg = SFPVariables, cex.axis = .75, ylab="Relative Importance")

saveRDS(PermSFPModel, file="../data_out/PermSFPModel.rds")
```

```{r single flavor plus pop predictions}

PredictSFPModel <- baggedlogistic(formula= glm.formula.SFP, traindata = training.data, cores=20, no.iterations = 1000) 

#parse it out to save to github
SFPPredPlot <- PredictSFPModel[[2]]
saveRDS(SFPPredPlot, file="../data_out/Predictions/SFPPredPlot.rds")
#save the whole thing locally
saveRDS(PredictSFPModel, file="../data_out/Predictions/PredictSFPModel.rds")
```

```{r}
glm.formula2 <- as.formula("case~  NDVI+NDVIScale+ 
                          popLog10+
                          RF+RFsqrt+
                          tempMean+tempScale+
                          spRich+primProp") 

PermTrimModel2 <- permOneVar(formula = glm.formula2,traindata = training.data, cores=3, no.iterations = 100 )
PermTrimAUC3 <- PermTrimModel2[[1]]

```

permutate to get a base AUC 
lagged rainfall (maybe not)
adding interactions (temp and rain)



# Visualizing model

This section is TBD. 

***

## Model prediction of testing data

*DON'T TOUCH UNTIL DONE WITH MODEL DEVELOPMENT*

The predictions for the testing data were made by building a similar bagged model (100 iterations using 10 presences and 100 background), but predicted on the testing data instead of the training data. 

```{r testModel, eval=FALSE}

cores.to.use <- 1
iterations <- 100

#create class to combine multiple results
multiResultClass <- function(predictions=NULL,coefs=NULL)
{
  me <- list(
    predictions = predictions,
    coefs = coefs
  )

  ## Set the name for the class
  class(me) <- append(class(me),"multiResultClass")
  return(me)
}


cl <- makeCluster(cores.to.use)
registerDoParallel(cl)

testModel <- foreach(i=1:iterations) %dopar% {
    result <- multiResultClass()
    output1 <- bagging(form.x.y=glm.formula,training.pres=train.pres,training.abs=train.bg,new.data=testing)
    result$predictions <-output1[[1]]
    result$coefs <- output1[[2]]
    return(result)
}

stopCluster(cl)

#pull out data in usable fashion
testingPreds <- do.call(cbind,(lapply(testModel, '[[', 1)))
testingCoefs <- do.call(cbind,(lapply(testModel, '[[', 2)))

```

```{r test AUC, eval=FALSE}
test.preds<- apply(testingPreds, 1, mean)
preds <- prediction(test.preds, testing$case)
# perf <- performance(preds, "sens", "spec")
# plot(perf)

perf2 <- performance(preds, "tpr", "fpr")
plot(perf2)

test.auc <- performance(preds, "auc") 
```

The model has an AUC of  when predicting the testing data. 



----

#Snippets from Ebola code

Example of using above function:

```{r, eval=FALSE}
#write glm formula
glm.formula<-as.formula("pres~rf.ym+evi+pet+rf.scaled+as.factor(pd.yr.binned)")

#predict on training set
training<-rbind(train.pres,train.bg)
output1<-bagging(form.x.y=glm.formula,training.pres=train.pres,training.abs=train.bg,new.data=training,iterations=176)

#predict on testing set
testing<-rbind(test.pres,test.bg)
output2<-bagging(form.x.y=glm.formula,training.pres=train.pres,training.abs=train.bg,new.data=testing,iterations=cores.to.use)
```

Giving it a test with fake data:

```{r, eval=FALSE}
trainTest <- cbind(c(rep(1, 30), rep(0,400)), c(rnorm(30,5,2), rnorm(400,10,4)), c(rnorm(30,100,20), rnorm(400,40,8)), c(rnorm(30,0.5, 0.1), rnorm(400,0.1,0.1)))
trainTest[trainTest<0] <- 0
colnames(trainTest) <- c("pres", "v1", "v2", "v3")
trainTest <- as.data.frame(trainTest)

#split into pres and abs
train.pres <- trainTest[trainTest$pres==1,]
train.abs <- trainTest[trainTest$pres==0,]

glm.formula <- as.formula("pres~v1+v2+v3")

cores.to.use <- 10 #bc my macbook sucks
cl <- makeCluster(cores.to.use)
registerDoParallel(cl)

output1 <- bagging(form.x.y=glm.formula,training.pres=train.pres,training.abs=train.abs,new.data=trainTest,iterations=cores.to.use)

stopCluster(cl) #this worked perfectly (all the positives got 9, and 0s got 0)






```

The bagging model sums up the predictions (in this case 9 of them), which then should be averaged.

After this, they then predicted over all of sub-Saharan Africa:

1. Combined all presence into one df, and all bg into one df

2. did bagged predictions using all p and all bg

3. each iteration was split by year, then run over all the cores, six times each

```{r}
# for (i in 1:12){
#   rf.scaled<-fread("rf.scaled.all.csv",select=((i*32)-31):(i*32)) 
#   rf.ym<-fread("rf.ym.df.csv",select=((i*32)-31):(i*32)) 
#   for (j in 1:32){
#     rf.scaled.step<-rf.scaled.all[,j] #Single year of data
#     rf.ym.step<-rf.ym.all[,j] #Single year of data
#     all.data.step<-as.data.frame(cbind(evi.pet.pd.binned,rf.scaled.step,log10(rf.ym.step+1)))
#     colnames(all.data.step)<-c("evi","pet","pd.yr.binned","rf.scaled","rf.ym")
#     sum<-NULL
#     chunks<-6
#     for (k in 1:chunks){
#       output<-bagging(form.x.y=glm.formula,train.pres=final.p,training.bg=final.b,new.data=all.data.step,iterations=cores.to.use) #what does k do here?? just runs 6 times
#       sum<-sum+output
#     }
#     mean.predict<-sum/(cores.to.use*chunks)
#     writeRDS(mean.predict,file=paste("Bags/predictionsSet",i*j))
#     print(i*j)
#   }
# }
```


